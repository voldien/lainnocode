{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "k49_mnist = \"/media/data-sets/kanji-mnist.zip\"\n",
    "\n",
    "\n",
    "def loadK49():\n",
    "\tsets = []\n",
    "\n",
    "\tfile_set = [\"k49-train-imgs.npz\", \"k49-train-labels.npz\", \"k49-test-imgs.npz\", \"k49-test-labels.npz\"]\n",
    "\twith zipfile.ZipFile(k49_mnist, 'r') as zip:\n",
    "\t\tfor i, path in enumerate(file_set):\n",
    "\t\t\tdata = zip.read(path)\n",
    "\t\t\twith zipfile.ZipFile(io.BytesIO(data), 'r') as f:\n",
    "\t\t\t\tdata_ = f.read(f.namelist()[0])\n",
    "\t\t\t\tsets.append(np.load(io.BytesIO(data_), allow_pickle=True))\n",
    "\treturn (sets[0], sets[1]), (sets[2], sets[3])\n",
    "\n",
    "\n",
    "kanji_mnist = \"/media/data-sets/kanji-mnist.zip\"\n",
    "\n",
    "\n",
    "def loadKanjiMNIST():\n",
    "\tsets = []\n",
    "\tfile_set = [\"kmnist-train-imgs.npz\", \"kmnist-train-labels.npz\", \"kmnist-test-imgs.npz\", \"kmnist-test-labels.npz\"]\n",
    "\twith zipfile.ZipFile(kanji_mnist, 'r') as zip:\n",
    "\t\tfor i, path in enumerate(file_set):\n",
    "\t\t\tdata = zip.read(path)\n",
    "\t\t\twith zipfile.ZipFile(io.BytesIO(data), 'r') as f:\n",
    "\t\t\t\tdata_ = f.read(f.namelist()[0])\n",
    "\t\t\t\tsets.append(np.load(io.BytesIO(data_), allow_pickle=True))\n",
    "\treturn (sets[0], sets[1]), (sets[2], sets[3])\n",
    "\n",
    "math_symbols = \"/media/data-sets/math_symbols.zip\"\n",
    "def loadMathSymbols():\n",
    "\twith zipfile.ZipFile(math_symbols, 'r') as zip:\n",
    "\t\tdirs = list(set([os.path.dirname(x) for x in zip.namelist()]))\n",
    "\t\tfor dir in dir:\n",
    "\t\t\tpass\n",
    "\t\t# for i, path in enumerate(file_set):\n",
    "\t\t# \tpass\n",
    "\treturn ()\n",
    "\n",
    "english_symbols = \"/media/data-sets/english_letters.zip\"\n",
    "def loadEnglishSymbols():\n",
    "\tpass\n",
    "\n",
    "\n",
    "def loadAllDataSet():\n",
    "\t(mnist_train_X, mnist_train_y), (mnist_test_X, mnist_test_y) = mnist.load_data()\n",
    "\tmnist_qu = [str(i) for i in range(0, 10)]\n",
    "\t(k49_mnist_train_X, k49_mnist_train_y), (k49_mnist_test_X, k49_mnist_test_y) = loadK49()\n",
    "\t(Kanji_mnist_train_X, Kanji_mnist_train_y), (Kanji_mnist_test_X, Kanji_mnist_test_y) = loadKanjiMNIST()\n",
    "\n",
    "\tquantive_labels = []\n",
    "\n",
    "\tlabelSets = []\n",
    "\tlabelSets.append(np.concatenate((mnist_train_y, mnist_test_y)))\n",
    "\tlabelSets.append(np.concatenate((k49_mnist_train_y, k49_mnist_test_y)))\n",
    "\tlabelSets.append(np.concatenate((Kanji_mnist_train_y, Kanji_mnist_test_y)))\n",
    "\n",
    "\tlabels = []\n",
    "\tnrLabels = 0\n",
    "\tfor labelset in labelSets:\n",
    "\t\tfor label in labelset:\n",
    "\t\t\tlabels.append(label + nrLabels)\n",
    "\t\tnrLabels += np.amax(labelset)\n",
    "\n",
    "\treturn np.concatenate((mnist_train_X, mnist_test_X, k49_mnist_train_X, k49_mnist_test_X, Kanji_mnist_train_X,\n",
    "\t\t\t\t\t\t   Kanji_mnist_test_X)), np.array(labels), quantive_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os.path\n",
    "\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import procdata\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\"\"\"# Loading and Process of data\"\"\"\n",
    "\n",
    "cache_path_dataX = \"data_X_cache_file.npz\"\n",
    "cache_path_dataY = \"data_Y_cache_file.npz\"\n",
    "if os.path.exists(cache_path_dataX):\n",
    "\ttrain_X = np.load(cache_path_dataX, allow_pickle=True)['arr_0']\n",
    "\ttrain_y = np.load(cache_path_dataY, allow_pickle=True)['arr_0']\n",
    "else:\n",
    "\ttrain_X, train_y, quantive_labels = procdata.loadAllDataSet()\n",
    "\ttrain_X = train_X.astype('float32') / 255.0\n",
    "\ttrain_X = np.expand_dims(train_X, axis=-1)  # <--- add batch axis\n",
    "\ttrain_y = np.expand_dims(train_y, axis=-1)  # <--- add batch axis\n",
    "\twith open(cache_path_dataX, 'wb') as f:\n",
    "\t\tnp.savez_compressed(f, train_X)\n",
    "\twith open(cache_path_dataY, 'wb') as f:\n",
    "\t\tnp.savez_compressed(f, train_y)\n",
    "\twith open(\"qunative_labels\") as f:\n",
    "\t\tf.write(quantive_labels)\n",
    "\n",
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 48\n",
    "\n",
    "\n",
    "def plotCostHistory(history, loss_label=\"\", val_label=\"\", title=\"\", x_label=\"\", y_label=\"\"):\n",
    "\tfor k, v in history.items():\n",
    "\t\tplt.plot(v, label=k)\n",
    "\tplt.title(label=title)\n",
    "\tplt.ylabel(ylabel=y_label)\n",
    "\tplt.xlabel(xlabel=x_label)\n",
    "\tplt.legend(loc=\"upper left\")\n",
    "\tplt.show()\n",
    "\tplt.imsave(title + \".png\")\n",
    "\n",
    "\n",
    "def model_process(trainX, trainY):\n",
    "\ttrain_X, test_X, train_y, test_y = train_test_split(trainX, trainY, shuffle=True, test_size=0.25, random_state=42)\n",
    "\n",
    "\t# Neuron network input and output.\n",
    "\timageShape = train_X[0].shape\n",
    "\toutput = np.amax(train_y) + 1\n",
    "\n",
    "\tprint('X_train: ' + str(train_X.shape))\n",
    "\tprint('Y_train: ' + str(train_y.shape))\n",
    "\tprint('X_test:  ' + str(test_X.shape))\n",
    "\tprint('Y_test:  ' + str(test_y.shape))\n",
    "\tprint(imageShape)\n",
    "\n",
    "\tplt.figure(figsize=(10, 4))\n",
    "\tplt.title(\"Example Data\")\n",
    "\tnrImage = 10\n",
    "\tfor index, (image, label) in enumerate(zip(train_X[0:nrImage], train_y[0:nrImage])):\n",
    "\t\tplt.subplot(2, 5, index + 1)\n",
    "\t\tplt.title(str.format(\"{0}\", label))\n",
    "\t\tplt.imshow(np.reshape(image, (28, 28)), cmap=plt.cm.gray)\n",
    "\tplt.show()\n",
    "\n",
    "\t\"\"\"# Common Functions\"\"\"\n",
    "\n",
    "\ttraining_size = len(train_X)\n",
    "\tvariance = 0.005\n",
    "\tvarThresh = VarianceThreshold(variance)\n",
    "\thigh_variance_training_data = train_X  # varThresh.fit(train_X)\n",
    "\thigh_variance_training_data_size = len(high_variance_training_data)\n",
    "\tprint(training_size)\n",
    "\tprint(high_variance_training_data_size)\n",
    "\n",
    "\t\"\"\"# Forward Neuron Network \"\"\"\n",
    "\n",
    "\tdef make_forward_neuron_network(hp):\n",
    "\t\t#\n",
    "\t\tmodel = tf.keras.Sequential()\n",
    "\t\tmodel.add(layers.Flatten(input_shape=imageShape))\n",
    "\n",
    "\t\tmodel.add(layers.LeakyReLU())\n",
    "\t\thp_rate = hp.Choice('rate', (0.1, 0.3, 0.5, 0.7, 0.9))\n",
    "\t\tmodel.add(layers.Dropout(rate=hp_rate))\n",
    "\n",
    "\t\thp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\t\tmodel.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "\n",
    "\t\thp_rate2 = hp.Choice('rate', (0.1, 0.3, 0.5, 0.7, 0.9))\n",
    "\t\tmodel.add(layers.Dropout(rate=hp_rate2))\n",
    "\t\tmodel.add(layers.Dense(output))\n",
    "\n",
    "\t\tmodel.summary()\n",
    "\n",
    "\t\thp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "\t\tmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "\t\t\t\t\t  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\t\treturn model\n",
    "\n",
    "\ttrain_dataset = tf.data.Dataset.from_tensor_slices(high_variance_training_data).shuffle(BUFFER_SIZE).batch(\n",
    "\t\tBATCH_SIZE)\n",
    "\n",
    "\ttuner = kt.Hyperband(make_forward_neuron_network,\n",
    "\t\t\t\t\t\t objective='val_accuracy',\n",
    "\t\t\t\t\t\t max_epochs=EPOCH,\n",
    "\t\t\t\t\t\t factor=3,\n",
    "\t\t\t\t\t\t directory='my_dir',\n",
    "\t\t\t\t\t\t project_name='intro_to_kt')\n",
    "\n",
    "\tstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\ttuner.search(train_X, train_y, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\tbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\tfnn_model = tuner.hypermodel.build(best_hps)\n",
    "\tfnn_model.summary()\n",
    "\n",
    "\tcheckpoint_filepath_forward_NN = os.path.join(\"checkpoints\", \"mnist_forward_neuron_network_ckpt\")\n",
    "\n",
    "\t# fnn_model.load_weights(checkpoint_filepath_forward_NN)\n",
    "\n",
    "\tmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\tfilepath=checkpoint_filepath_forward_NN,\n",
    "\t\tsave_weights_only=True,\n",
    "\t\tmonitor='accuracy',\n",
    "\t\tsave_freq='epoch',\n",
    "\t\tmode='max',\n",
    "\t\tsave_best_only=True)\n",
    "\n",
    "\ttf.keras.utils.plot_model(\n",
    "\t\tfnn_model, to_file='forward_model.png', show_shapes=True, show_dtype=True,\n",
    "\t\tshow_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,\n",
    "\t\tlayer_range=None\n",
    "\t)\n",
    "\n",
    "\tfnn_history = fnn_model.fit(train_X, train_y, epochs=EPOCH,\n",
    "\t\t\t\t\t\t\t\tbatch_size=BATCH_SIZE, validation_split=0.2)\n",
    "\n",
    "\tval_acc_per_epoch = fnn_history.history['val_accuracy']\n",
    "\tbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "\tprint('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "\t#\n",
    "\t# forward_nn_history = forward_neuron_network_model.fit(high_variance_training_data, train_y, epochs=EPOCH,\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t  batch_size=BATCH_SIZE, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "\t# Save the model for being reuse in other programs and etc.\n",
    "\tfnn_model.save('fnn_mnist.h5')\n",
    "\tfnn_model.save('fnn_mnist')\n",
    "\n",
    "\tfnn_eval_result = fnn_model.evaluate(test_X, test_y, verbose=2)\n",
    "\tprint(\"[test loss, test accuracy]:\", fnn_eval_result)\n",
    "\n",
    "\tplotCostHistory(fnn_history.history, title=\"FNN Performance History\")\n",
    "\n",
    "\t\"\"\"# Convolution Neuron Network\"\"\"\n",
    "\n",
    "\n",
    "def compute_cnn_model(trainX, trainY):\n",
    "\ttrain_X, test_X, train_y, test_y = train_test_split(trainX, trainY, shuffle=True, test_size=0.25,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=42)\n",
    "\n",
    "\t# Neuron network input and output.\n",
    "\timageShape = train_X[0].shape\n",
    "\toutput = np.amax(train_y) + 1\n",
    "\n",
    "\tprint('X_train: ' + str(train_X.shape))\n",
    "\tprint('Y_train: ' + str(train_y.shape))\n",
    "\tprint('X_test:  ' + str(test_X.shape))\n",
    "\tprint('Y_test:  ' + str(test_y.shape))\n",
    "\tprint(imageShape)\n",
    "\n",
    "\tdef make_cnn_model(hp):\n",
    "\t\tcnn_model = tf.keras.Sequential()\n",
    "\t\t#\n",
    "\t\tkernel_init = hp.Choice('kernel_initializer', ['uniform', 'lecun_uniform', 'normal', 'zero',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'])\n",
    "\t\thp_kernel_sizes = hp.Int('filters', min_value=16, max_value=96, step=16)\n",
    "\t\tcnn_model.add(\n",
    "\t\t\tlayers.Conv2D(hp_kernel_sizes, (3, 3), kernel_initializer=kernel_init, activation='relu', padding='same',\n",
    "\t\t\t\t\t\t  input_shape=imageShape))\n",
    "\t\tcnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\t\t#\n",
    "\t\tcnn_model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=kernel_init, ))\n",
    "\t\tcnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\t\tcnn_model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=kernel_init, ))\n",
    "\n",
    "\t\tcnn_model.add(layers.Dropout(0.3))\n",
    "\n",
    "\t\tcnn_model.add(layers.Dense(64, activation=tf.nn.relu, kernel_initializer=kernel_init, ))\n",
    "\t\tcnn_model.add(layers.Flatten())\n",
    "\n",
    "\t\thp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\t\tcnn_model.add(layers.Dense(units=hp_units, activation='relu', kernel_initializer=kernel_init, ))\n",
    "\n",
    "\t\tcnn_model.add(layers.Dense(output))\n",
    "\n",
    "\t\tcnn_model.summary()\n",
    "\n",
    "\t\thp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "\t\tcnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "\t\t\t\t\t\t  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\t\treturn cnn_model\n",
    "\n",
    "\tBUFFER_SIZE = 256\n",
    "\tBATCH_SIZE = 64\n",
    "\n",
    "\tloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\toptimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "\ttuner = kt.Hyperband(make_cnn_model,\n",
    "\t\t\t\t\t\t objective='val_accuracy',\n",
    "\t\t\t\t\t\t max_epochs=EPOCH,\n",
    "\t\t\t\t\t\t factor=3,\n",
    "\t\t\t\t\t\t directory='cache',\n",
    "\t\t\t\t\t\t project_name='cnn')\n",
    "\n",
    "\tstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\ttuner.search(train_X, train_y, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\tbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "\tcnn_model = tuner.hypermodel.build(best_hps)\n",
    "\tcnn_model.summary()\n",
    "\tcnn_model_history = cnn_model.fit(train_X, train_y, epochs=EPOCH, validation_data=(test_X, test_y),\n",
    "\t\t\t\t\t\t\t\t\t  validation_split=0.2)\n",
    "\n",
    "\ttf.keras.utils.plot_model(\n",
    "\t\tcnn_model, to_file='forward_model.png', show_shapes=True, show_dtype=True,\n",
    "\t\tshow_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,\n",
    "\t\tlayer_range=None\n",
    "\t)\n",
    "\n",
    "\tval_acc_per_epoch = cnn_model_history.history['val_accuracy']\n",
    "\tbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "\tprint('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "\tcnn_model.save(\"cnn_mnist.h5\")\n",
    "\n",
    "\tcnn_model.evaluate(test_X, test_y, verbose=2)\n",
    "\n",
    "\tcnn_model.summary()\n",
    "\n",
    "\tplotCostHistory(cnn_model_history.history, title=\"CNN Performance History\")\n",
    "\n",
    "\n",
    "model_process(train_X, train_y)\n",
    "compute_cnn_model(train_X, train_y)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
