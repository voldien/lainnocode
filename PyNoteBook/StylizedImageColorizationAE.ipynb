{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylized Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/media/programming/projects/machineLearning/ML/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q imageio pydot tensorflow-gpu==2.9.1 keras matplotlib graphviz moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras_tuner as kt\n",
    "from numpy import asarray\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os.path\n",
    "import math\n",
    "import imageio.v2 as imageio\n",
    "import glob\n",
    "from skimage.color import lab2rgb\n",
    "import PIL\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all Modules and Configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:11:38.051036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:38.074119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:38.074506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    #mixed_precision.set_global_policy('mixed_float16')\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_directory(data_dir, train_subdir, test_subdir, image_size, batch_size, train_size=0.8):\n",
    "    def configure_for_performance(ds, AUTOTUNE, shuffleSize):\n",
    "        ds = ds.cache()\n",
    "        if shuffleSize > 0:\n",
    "            ds = ds.shuffle(buffer_size=shuffleSize, reshuffle_each_iteration=False)\n",
    "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    data_train_dir = pathlib.Path(data_dir.as_posix() + \"/\" + train_subdir)\n",
    "    image_count = len(list(data_train_dir.glob('**/*.??g')))\n",
    "    print(\"{0}: Found {1} files\".format(data_train_dir, image_count))\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_train_dir,\n",
    "        interpolation='bilinear',\n",
    "        color_mode='rgb',\n",
    "        label_mode=None,\n",
    "        follow_links=True,\n",
    "        shuffle=False,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    #\n",
    "    data_test_dir = pathlib.Path(data_dir.as_posix() + \"/\" + test_subdir)\n",
    "    image_count = len(list(data_test_dir.glob('**/*.??g')))\n",
    "    print(\"{0}: Found {1} files\".format(data_test_dir, image_count))\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_test_dir,\n",
    "        interpolation='bilinear',\n",
    "        color_mode='rgb',\n",
    "        label_mode=None,\n",
    "        follow_links=True,\n",
    "        shuffle=False,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    #\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1.0 / 255.0)\n",
    "\n",
    "    @tf.function\n",
    "    def preprocess_lab(img):\n",
    "        image = tf.cast(img, tf.float16)\n",
    "\n",
    "        lab = tfio.experimental.color.rgb_to_lab(image)\n",
    "        l = lab[..., 0]\n",
    "\n",
    "        l = tf.expand_dims(l, axis=-1)\n",
    "        return tf.cast(tf.concat(l, axis=-1), tf.float16)\n",
    "\n",
    "    @tf.function\n",
    "    def preprocess_lab2(img):\n",
    "        image = tf.cast(img, tf.float16)\n",
    "        lab = tfio.experimental.color.rgb_to_lab(image)\n",
    "        a = lab[..., 1]\n",
    "        b = lab[..., 2]\n",
    "\n",
    "        a = tf.expand_dims(a, axis=-1)\n",
    "        b = tf.expand_dims(b, axis=-1)\n",
    "        return tf.cast(tf.concat([a, b], axis=-1), tf.float16)\n",
    "\n",
    "    nrBatches = image_count / batch_size\n",
    "\n",
    "    # Translate [0,255] -> [-128, 128]\n",
    "    normalized_train_ds = (train_ds.map(lambda x: preprocess_lab(normalization_layer(x))))\n",
    "\n",
    "    # Translate [0,255] -> [-1, 1]\n",
    "    normalized_test_ds = (test_ds.map(lambda x: (preprocess_lab2(normalization_layer(x)) * (1.0 / 128.0))))\n",
    "\n",
    "    train_ds = tf.data.Dataset.zip((normalized_train_ds, normalized_test_ds))\n",
    "    train_ds = train_ds.take(int(train_size * nrBatches))\n",
    "    test_ds = train_ds.skip(int(train_size * nrBatches)).take(int((1.0 - train_size) * nrBatches))\n",
    "\n",
    "    return configure_for_performance(train_ds, AUTOTUNE, 0), configure_for_performance(test_ds, AUTOTUNE, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-colorize: Found 92219 files\n",
      "Found 92219 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:11:55.778917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 19:11:55.779791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:55.779958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:55.780071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:56.162202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:56.162355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:56.162464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 19:11:56.162561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5026 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-colorize: Found 92219 files\n",
      "Found 92219 files belonging to 1 classes.\n",
      "Number of batches 9798 of 8 elements\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = (128, 128)\n",
    "EPOCHS = 128\n",
    "\n",
    "data_directory_path = \"data-colorize/\"\n",
    "data_dir = pathlib.Path(data_directory_path)\n",
    "\n",
    "train_images, validation_images = load_from_directory(data_dir, \"\", \"\", IMAGE_SIZE, BATCH_SIZE, 0.85)\n",
    "\n",
    "print(\"Number of batches {0} of {1} elements\".format(\n",
    "    len(train_images), BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 9\n",
    "\n",
    "# nrCol = 5\n",
    "# plt.figure(figsize=(10 * 2, 12))\n",
    "# for images in train_images.take(1):\n",
    "#     for i in range(0, 5):\n",
    "#         trainImage, expectedImage = images\n",
    "#         # Transform pixel values from [-1,1] to [0,1]\n",
    "#         trainLAB = trainImage[i, :, :].numpy().astype(dtype='float32')\n",
    "#         ax = plt.subplot(4, nrCol, nrCol * 0 + i + 1)\n",
    "#         plt.imshow(trainLAB, cmap='gray')\n",
    "#         plt.axis(\"off\")\n",
    "#         ax = plt.subplot(4, nrCol, nrCol * 1 + i + 1)\n",
    "#         plt.imshow(expectedImage[i, :, :, 0], cmap='Blues')\n",
    "#         plt.axis(\"off\")\n",
    "#         ax = plt.subplot(4, nrCol, nrCol * 2 + i + 1)\n",
    "#         plt.imshow(expectedImage[i, :, :, 1], cmap='Greens')\n",
    "#         plt.axis(\"off\")\n",
    "#         expectedImage = expectedImage[i, :, :, :].numpy().astype(dtype='float32') * 128\n",
    "#         RGBImage = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "#         RGBImage[:, :, 0] = trainLAB[:, :, 0]\n",
    "#         RGBImage[:, :, 1:] = expectedImage\n",
    "#         rgbExpected = lab2rgb(RGBImage)\n",
    "#         ax = plt.subplot(4, nrCol, nrCol * 3 + i + 1)\n",
    "#         plt.imshow(rgbExpected)\n",
    "#         plt.axis(\"off\")\n",
    "# plt.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(hp, input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    kernel_init = hp.Choice('kernel_initializer', ['uniform', 'lecun_uniform', 'normal',\n",
    "                                                   'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'])\n",
    "\n",
    "    model.add(layers.Input(input_shape))\n",
    "\n",
    "    num_layers = hp.Int('generator_number_layers', min_value=1, max_value=3, step=1)\n",
    "\n",
    "    hp_kernel_filter_size_l0 = hp.Int('kernel_filter_downscale', min_value=5, max_value=7, step=1)\n",
    "\n",
    "    for i in range(0, num_layers):\n",
    "        filter_size = 2 ** (i + hp_kernel_filter_size_l0)\n",
    "\n",
    "        model.add(layers.Conv2D(filter_size, (3, 3), padding='same', kernel_initializer=kernel_init))\n",
    "        model.add(layers.ReLU(dtype='float32'))\n",
    "        model.add(layers.Conv2D(filter_size, (3, 3), padding='same', strides=2))\n",
    "        model.add(layers.ReLU(dtype='float32'))\n",
    "\n",
    "    hp_kernel_filter_size_latent = (num_layers + hp_kernel_filter_size_l0) + 1\n",
    "    #latent_space = hp.Int('latent_space', min_value=64, max_value=1024, step=64)\n",
    "\n",
    "    #\n",
    "    encoder_last_conv2 = layers.Conv2D(2 ** hp_kernel_filter_size_latent, (3, 3), padding='same', kernel_initializer=kernel_init)\n",
    "    model.add(encoder_last_conv2)\n",
    "    model.add(layers.ReLU(dtype='float32'))\n",
    "\n",
    "    # Latent Space\n",
    "    # model.add(layers.Flatten())\n",
    "    # TOOD add extra latent space dim to allow for seed.\n",
    "    # model.add(layers.Dense(latent_space))\n",
    "    # model.add(layers.ReLU(dtype='float32'))\n",
    "    #latent_space_filter_size = 2 ** (hp_kernel_filter_size_latent - 1)\n",
    "    #model.add(layers.Dense(latent_space_filter_size * encoder_last_conv2.output_shape[1] * encoder_last_conv2.output_shape[2]))\n",
    "    # model.add(layers.ReLU(dtype='float32'))\n",
    "    #model.add(layers.Reshape(target_shape=(encoder_last_conv2.output_shape[1], encoder_last_conv2.output_shape[2], latent_space_filter_size)))\n",
    "\n",
    "    #\n",
    "    model.add(layers.Conv2D(2 ** (hp_kernel_filter_size_latent - 1), (3, 3), padding='same', kernel_initializer=kernel_init))\n",
    "    model.add(layers.ReLU(dtype='float32'))\n",
    "\n",
    "    hp_kernel_filter_size_l1 = hp_kernel_filter_size_l0\n",
    "\n",
    "    for i in range(0, num_layers):\n",
    "        filter_size = 2 ** (hp_kernel_filter_size_l1 - i)\n",
    "\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(filter_size, (3, 3), padding='same', kernel_initializer=kernel_init))\n",
    "        model.add(layers.ReLU(dtype='float32'))\n",
    "\n",
    "    model.add(layers.Conv2D(2, (3, 3), padding='same', kernel_initializer=kernel_init))\n",
    "    model.add(layers.Activation(activation='tanh', dtype='float32'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(hp, input_size):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    kernel_init = hp.Choice('kernel_initializer', ['uniform', 'lecun_uniform', 'normal',\n",
    "                                                   'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'])\n",
    "\n",
    "    n_layers = hp.Int('discriminator_number_layers', min_value=1, max_value=2, step=1)\n",
    "\n",
    "    #\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), use_bias=False, padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(layers.BatchNormalization(dtype='float32'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2, dtype='float32'))\n",
    "\n",
    "    max_filters_size = 1024\n",
    "\n",
    "    #\n",
    "    for i in range(0, n_layers):\n",
    "        filter_size = 128 * (2 ** i)\n",
    "        filter_size = min(max_filters_size, filter_size)\n",
    "        kernel_size = (5, 5)\n",
    "        #\n",
    "        model.add(layers.Conv2D(filter_size, kernel_size=kernel_size,\n",
    "                                strides=(2, 2), use_bias=False, padding='same', kernel_initializer=kernel_init))\n",
    "        model.add(layers.BatchNormalization(dtype='float32'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2, dtype='float32'))\n",
    "\n",
    "    model.add(layers.Conv2D(1, kernel_size=(4, 4), strides=(\n",
    "        2, 2), padding='valid', use_bias=False, kernel_initializer=kernel_init))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid',dtype='float32'))\n",
    "    #model.add(layers.Activation('sigmoid', dtype='float32'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp, image_size):\n",
    "\n",
    "    class DCGAN(keras.Model):\n",
    "\n",
    "        def __init__(self, generator_model, discriminator_model, **kwargs):\n",
    "            super(DCGAN, self).__init__(**kwargs)\n",
    "            self.generator = generator_model\n",
    "            self.discriminator = discriminator_model\n",
    "            self.latent_space_size = generator_model.input_shape\n",
    "\n",
    "            self.cross_entropy = tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=False)\n",
    "\n",
    "            self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "            self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "            self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "        def generator_loss(self, fake_output):\n",
    "\n",
    "            return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "        def discriminator_loss(self, real_output, fake_output, smooth=0.11):\n",
    "\n",
    "            # label for real image is (1-smooth)\n",
    "            real_loss = self.cross_entropy(tf.ones_like(\n",
    "                real_output)*(1-smooth), real_output)\n",
    "            fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "            total_loss = real_loss + fake_loss\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "            super(DCGAN, self).compile(**kwargs)\n",
    "            self.generator_optimizer = generator_optimizer\n",
    "            self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        def summary(self):\n",
    "            self.generator.summary()\n",
    "            self.discriminator.summary()\n",
    "\n",
    "        def fit(self, *args, **kwargs):\n",
    "            self.validation_data = kwargs.get(\"validation_data\")\n",
    "            self.batch_size = kwargs.get(\"batch_size\")\n",
    "            return super(DCGAN, self).fit(*args, **kwargs)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(self, data):\n",
    "\n",
    "            #batch_size = tf.shape(data)[0]\n",
    "\n",
    "            # Extract train and result data.\n",
    "            trainX, trainY = data\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                # generate image using generator\n",
    "                generated_image = self.generator(trainX, training=True)\n",
    "\n",
    "                # discriminator's prediction for real image\n",
    "                real_output = self.discriminator(trainY, training=True)\n",
    "\n",
    "                # discriminator's estimate for fake image\n",
    "                fake_output = self.discriminator(\n",
    "                    generated_image, training=True)\n",
    "\n",
    "                # compute loss\n",
    "                gen_loss = self.generator_loss(fake_output)\n",
    "                disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                # optimize generator first\n",
    "                generator_grad = gen_tape.gradient(\n",
    "                    gen_loss, self.generator.trainable_variables)\n",
    "                discriminator_grad = disc_tape.gradient(\n",
    "                    disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                # optimize discriminator after generator\n",
    "                self.generator_optimizer.apply_gradients(\n",
    "                    zip(generator_grad, self.generator.trainable_variables))\n",
    "                self.discriminator_optimizer.apply_gradients(\n",
    "                    zip(discriminator_grad, self.discriminator.trainable_variables))\n",
    "\n",
    "                self.gen_loss_tracker.update_state(gen_loss)\n",
    "                self.disc_loss_tracker.update_state(disc_loss)\n",
    "                self.acc_tracker.update_state(tf.math.reduce_mean(fake_output))\n",
    "\n",
    "            result = {\n",
    "                \"loss\": disc_loss,\n",
    "                \"accuracy\": tf.math.reduce_mean(fake_output),\n",
    "                \"generator_loss\": gen_loss,\n",
    "                \"discriminator_loss\": disc_loss\n",
    "            }\n",
    "            # if self.validation_data:\n",
    "            #     for batchValidation in self.validation_data:\n",
    "            #         trainValX, trainValY = batchValidation\n",
    "            #         generated_image = self.generator(trainX, training=False)\n",
    "            #         real_output = self.discriminator(trainY, training=False)\n",
    "            #         fake_output = self.discriminator(\n",
    "            #                 generated_image, training=False)\n",
    "            #         gen_val_loss = self.generator_loss(fake_output)\n",
    "            #         disc_val_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "            #         result[\"val_loss\"] = disc_val_loss\n",
    "            #         result[\"val_accuracy\"] = tf.math.reduce_mean(fake_output)\n",
    "            #         # Return a dict mapping metric names to current value.\n",
    "            #         # Note that it will include the loss (tracked in self.metrics).\n",
    "\n",
    "            return result\n",
    "\n",
    "        def test_step(self, data):\n",
    "            # Unpack the data\n",
    "            trainValX, trainValY = data\n",
    "            generated_image = self.generator(trainValX, training=False)\n",
    "            real_output = self.discriminator(trainValY, training=True)\n",
    "            fake_output = self.discriminator(\n",
    "                generated_image, training=False)\n",
    "            gen_val_loss = self.generator_loss(fake_output)\n",
    "            disc_val_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "            result[\"val_loss\"] = disc_val_loss\n",
    "            result[\"val_accuracy\"] = tf.math.reduce_mean(fake_output)\n",
    "            # Return a dict mapping metric names to current value.\n",
    "            # Note that it will include the loss (tracked in self.metrics).\n",
    "            return result\n",
    "            # return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [self.gen_loss_tracker, self.disc_loss_tracker, self.acc_tracker]\n",
    "\n",
    "    # Create the models.\n",
    "    discriminator = make_discriminator_model(hp, (image_size[0], image_size[1], 2))\n",
    "    generator = create_generator(hp, (image_size[0], image_size[1], 1))\n",
    "\n",
    "    #\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[2e-2, 2e-3, 2e-4])\n",
    "    #hp_generator_optimizer = hp.Choice('generator_optimizer', ['sgd', 'adam', 'rmsprop'])\n",
    "    #hp_discriminator_optimizer = hp.Choice('discriminator_optimizer', ['sgd', 'adam', 'rmsprop'])\n",
    "#\n",
    "    ## #\n",
    "    #generator_optimizer = tf.keras.optimizers.get(hp_generator_optimizer)\n",
    "    #discriminator_optimizer = tf.keras.optimizers.get(hp_discriminator_optimizer)\n",
    "#\n",
    "    ##\n",
    "    #generator_optimizer.learning_rate = hp_learning_rate\n",
    "    #discriminator_optimizer.learning_rate = hp_learning_rate\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=hp_learning_rate, beta_1=0.5)\n",
    "\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=hp_learning_rate, beta_1=0.5)\n",
    "\n",
    "    #\n",
    "    dcgan = DCGAN(generator, discriminator)\n",
    "    dcgan.compile(generator_optimizer, discriminator_optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return dcgan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Discrimintor First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 01m 51s]\n",
      "accuracy: 1.386023996019503e-06\n",
      "\n",
      "Best accuracy So Far: 0.00033732742303982377\n",
      "Total elapsed time: 00h 03m 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:19:36.112886: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "uniform           |he_normal         |kernel_initializer\n",
      "2                 |2                 |discriminator_number_layers\n",
      "1                 |1                 |generator_number_layers\n",
      "7                 |6                 |kernel_filter_downscale\n",
      "0.0002            |0.0002            |learning_rate\n",
      "\n",
      "Epoch 1/128\n",
      "   6/1000 [..............................] - ETA: 1:01 - loss: 1.7036 - accuracy: 0.0511 - generator_loss: 2.9890 - discriminator_loss: 0.8169WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0270s vs `on_train_batch_end` time: 0.0343s). Check your callbacks.\n",
      " 693/1000 [===================>..........] - ETA: 17s - loss: 0.4693 - accuracy: 0.0152 - generator_loss: 4.8170 - discriminator_loss: 0.4212"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    def model_builder(hp):\n",
    "        model = create_model(hp, IMAGE_SIZE)\n",
    "        return model\n",
    "\n",
    "    #tuner = kt.Hyperband(model_builder,\n",
    "    #                     objective=kt.Objective(\"accuracy\", direction=\"max\"),\n",
    "    #                     max_epochs=16,\n",
    "    #                     factor=6,\n",
    "    #                     directory='cache',\n",
    "    #                     project_name='Stylized Image Colorization')\n",
    "    tuner = kt.BayesianOptimization(model_builder,\n",
    "                        objective=kt.Objective(\"accuracy\", direction=\"max\"),\n",
    "                         max_trials=10,\n",
    "                         overwrite=True,\n",
    "                         directory='cache',\n",
    "                         project_name='Stylized Image Colorization')\n",
    "\n",
    "    #\n",
    "    stop_generator_loss_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='generator_loss', patience=1, min_delta=0.05)\n",
    "    stop_discriminator_loss_early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='discriminator_loss', patience=1, min_delta=0.05)\n",
    "\n",
    "    tuner.search(train_images.take(1000), validation_data=validation_images, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                callbacks=[stop_generator_loss_early, stop_discriminator_loss_early, tf.keras.callbacks.TerminateOnNaN()\n",
    "                           ], verbose=1)\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    model = model_builder(best_hps)\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResult(model, batchImage, maxNumImages=6):\n",
    "    output = model.predict(batchImage)\n",
    "    output = output * 128\n",
    "    nrElements = min(len(output), maxNumImages)\n",
    "\n",
    "    fig = plt.figure(figsize=(maxNumImages * 2, 4 * 2))\n",
    "    for image in batchImage:\n",
    "        for i in range(nrElements):\n",
    "            trainImage, expectedImage = image\n",
    "\n",
    "            canvas = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "            canvas[:, :, 0] = trainImage[i][:, :, 0]\n",
    "            canvas[:, :, 1:] = expectedImage[i][:, :, :] * 128\n",
    "\n",
    "            ax = plt.subplot(4, maxNumImages, i + 1)\n",
    "            plt.imshow((asarray(lab2rgb(canvas)).astype(dtype='float32')))\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            ax = plt.subplot(4, maxNumImages, maxNumImages * 1 + i + 1)\n",
    "            plt.imshow(output[i, :, :, 0], cmap='Blues')\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            ax = plt.subplot(4, maxNumImages, maxNumImages * 2 + i + 1)\n",
    "            plt.imshow(output[i, :, :, 1], cmap='Greens')\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            canvas = np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "            canvas[:, :, 0] = trainImage[i][:, :, 0]\n",
    "            canvas[:, :, 1:] = output[i][:, :, :]\n",
    "\n",
    "            ax = plt.subplot(4, maxNumImages, maxNumImages * 3 + 1 + i)\n",
    "            plt.imshow(asarray(lab2rgb(canvas)).astype(dtype='float32'))\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "class save_images(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, trainData, **kwargs):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__(**kwargs)\n",
    "        self.trainData = trainData\n",
    "        self.trainSet = self.trainData.take(1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        fig = showResult(self.model.generator, self.trainSet)\n",
    "        fig.savefig(\"StylizedColorization{0}.png\".format(epoch))\n",
    "        fig = showResult(self.model.generator, self.trainSet, 12)\n",
    "        fig.savefig(\"StylizedColorizationBig{0}.png\".format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 19:10:48.813345: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n",
      "2022-06-21 19:10:49.699323: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-21 19:10:49.824901: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-21 19:10:50.115077: W tensorflow/core/common_runtime/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 680/9798 [=>............................] - ETA: 3:56 - loss: 1.4790 - accuracy: 0.4608 - generator_loss: 0.8041 - discriminator_loss: 1.5666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(checkpoint_path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=13'>14</a>\u001b[0m     model\u001b[39m.\u001b[39mload_weights(checkpoint_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=15'>16</a>\u001b[0m model_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=16'>17</a>\u001b[0m                           epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=17'>18</a>\u001b[0m                           validation_data\u001b[39m=\u001b[39;49mvalidation_images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=18'>19</a>\u001b[0m                           verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[cp_callback, save_images(train_images)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000018?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39msave_weights(checkpoint_path)\n",
      "\u001b[1;32m/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb Cell 14'\u001b[0m in \u001b[0;36mcreate_model.<locals>.DCGAN.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000013?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_data \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalidation_data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000013?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000013?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(DCGAN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    602\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1126\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = \"checkpoints/training_stylized_colorization/cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path)).expect_partial()\n",
    "\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "model_history = model.fit(train_images,\n",
    "                          epochs=EPOCHS,\n",
    "                          validation_data=validation_images,\n",
    "                          verbose=1, callbacks=[cp_callback, save_images(train_images)])\n",
    "\n",
    "model.save_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/PyNoteBook/ImageColorizationStylized.ipynb#ch0000021?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mgan-colorization-stylized.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/media/programming/projects/machineLearning/ML/venv/lib/python3.9/site-packages/keras/saving/save.py:142\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m (save_format \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mh5\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     (h5py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, h5py\u001b[39m.\u001b[39mFile)) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     saving_utils\u001b[39m.\u001b[39mis_hdf5_filepath(filepath)):\n\u001b[1;32m    139\u001b[0m   \u001b[39m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m  \u001b[39m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    141\u001b[0m       \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, sequential\u001b[39m.\u001b[39mSequential)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msubclassed models, because such models are defined via the body of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[39m'\u001b[39m\u001b[39ma Python method, which isn\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt safely serializable. Consider saving \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mto the Tensorflow SavedModel format (by setting save_format=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mor using `save_weights`.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    149\u001b[0m   hdf5_format\u001b[39m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    150\u001b[0m       model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save(\"gan-colorization-stylized.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evolution\n",
    "result = model.evaluate(validation_images, batch_size=BATCH_SIZE)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "\n",
    "display_image(EPOCHS - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'stylized-colorizing.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('StylizedColorizationBig*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "Image(url=anim_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCostHistory(history, loss_label=\"\", val_label=\"\", title=\"\", x_label=\"\", y_label=\"\"):\n",
    "    fig = plt.figure()\n",
    "    for k, v in history.items():\n",
    "        plt.plot(v, label=k)\n",
    "    plt.title(label=title)\n",
    "    plt.ylabel(ylabel=y_label)\n",
    "    plt.xlabel(xlabel=x_label)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    fig.show()\n",
    "    fig.savefig(title + \".png\")\n",
    "\n",
    "\n",
    "plotCostHistory(model_history.history, title=str.format(\n",
    "    \"{0} Performance History\", \"Stylized Colorization\"))\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model, to_file=str.format('{0}.png', \"Stylized Colorization\"), show_shapes=True, show_dtype=True,\n",
    "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,\n",
    "    layer_range=None\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9705cd9bb1cd3f75cacc64c830c816f4d5b8b46bb8973c8b095f871cd13babda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
