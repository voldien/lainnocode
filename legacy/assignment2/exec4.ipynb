{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from graph import mesh2Dgrid\n",
    "from regression import logisticGradient, logisticCost, extendMatrix, linear, Sigmod, mapFeature\n",
    "from statistics import normalizeFeature\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "np.set_printoptions(precision=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degress = [i for i in range(1, 10)]\n",
    "h = .05  # step size in the mesh\n",
    "threshold = 0.5\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])  # mesh plot\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])  # colors\n",
    "\n",
    "\n",
    "def getColor(y):\n",
    "\treturn ['r', 'b'][y]\n",
    "\n",
    "\n",
    "def mapExpandedList(X):\n",
    "\treturn [X[0], X[1], X[1] ** 2, X[0] * X[1]]\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"datasets/microchips.csv\")\n",
    "\n",
    "# Train Data.\n",
    "X = np.array(normalizeFeature(np.array(df.drop(['id'], axis=1).T)))\n",
    "y = np.array(df['id'])\n",
    "\n",
    "# Linear logistic function.\n",
    "Xe = extendMatrix(mapExpandedList(X), False)\n",
    "# Get the shape of the beta.\n",
    "linearBeta = np.zeros(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the best theta with gradient descent (alpha, n)\n",
    "cost_ai = []\n",
    "for alpha in [1.0 / i for i in range(10, 5000, 1000)]:\n",
    "\tfor n in range(10, 1000, 100):\n",
    "\t\toptimized = logisticGradient(Xe, linearBeta, y, alpha, n)\n",
    "\t\tcombinationCost = logisticCost(Xe, optimized, y, linear)\n",
    "\n",
    "\t\t# Results.\n",
    "\t\tprint(\"alpha {}, iterations: {} J(B) : {}\".format(alpha, n, round(combinationCost, 7)))\n",
    "\n",
    "\t\tcost_ai.append([[n, alpha], combinationCost])\n",
    "\n",
    "# Create graph of cost vs iterations\n",
    "# Extract the best fit beta\n",
    "XeB = sorted(cost_ai, key=itemgetter(1))\n",
    "error = XeB[0][1]\n",
    "n, j = XeB[0][0]\n",
    "print(\"The best cost {} with ({},{})\".format(error, n, j))\n",
    "lineX = []\n",
    "lineY = []\n",
    "for n in range(1, 1000, 10):\n",
    "\toptimized = logisticGradient(Xe, linearBeta, y, j, n)\n",
    "\tcombinationCost = logisticCost(Xe, optimized, y, linear)\n",
    "\tlineX.append(n)\n",
    "\tlineY.append(combinationCost)\n",
    "\n",
    "plti = plt.subplot(1, 2, 1)\n",
    "plti.plot(lineX, lineY)\n",
    "plti.set_xlabel(\"Iterations\")\n",
    "plti.set_ylabel(\"Cost\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.T, y, test_size=0.25, random_state=0, shuffle=True)\n",
    "\n",
    "# Create graph of decision boundary.\n",
    "plti = plt.subplot(1, 2, 2)\n",
    "\n",
    "X1 = X[0]\n",
    "X2 = X[1]\n",
    "\n",
    "#\n",
    "xx, yy, x1, x2 = mesh2Dgrid((-2, -2), (2, 2), h)\n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "Xdeg = poly.fit_transform(x_train)\n",
    "\n",
    "#\n",
    "map = poly.fit_transform([[_x, _y] for _x, _y in zip(x1, x2)])\n",
    "linearBeta = np.ones(len(Xdeg[0]))\n",
    "optimized = logisticGradient(Xdeg, linearBeta, y_train, j, n)\n",
    "p = np.array(Sigmod(np.dot(map, optimized)))  # classify mesh ==> probabilities\n",
    "classes = p > threshold  # round off probabilities\n",
    "clz_mesh = classes.reshape(xx.shape)  # return to mesh format\n",
    "\n",
    "# Draw mesh and points.\n",
    "plti.pcolormesh(xx, yy, clz_mesh, cmap=cmap_light)\n",
    "for x_, y_ in zip(X.T, y):\n",
    "\tplti.scatter(x_[0], x_[1], color=getColor(y_), cmap=cmap_bold)\n",
    "\n",
    "plti.set_xlabel(\"X\")\n",
    "plti.set_ylabel(\"Y\")\n",
    "\n",
    "plt.suptitle(\"microchips.csv - Logistic Regression (Redefined)\")\n",
    "plt.show()\n",
    "\n",
    "# Repeat 2\n",
    "lineX = []\n",
    "lineY = []\n",
    "for n in range(1, 1000, 5):\n",
    "\tregression = LogisticRegression(max_iter=n, n_jobs=4, C=j, verbose=1, solver='lbfgs', random_state=0)\n",
    "\tregression.fit(x_train, y_train)\n",
    "\tcost = regression.score(x_test, y_test)\n",
    "\tlineX.append(n)\n",
    "\tlineY.append(cost)\n",
    "\n",
    "plti = plt.subplot(1, 2, 1)\n",
    "plti.plot(lineX, lineY)\n",
    "plti.set_xlabel(\"Iterations\")\n",
    "plti.set_ylabel(\"Cost\")\n",
    "\n",
    "# Create graph of decision boundary.\n",
    "plti = plt.subplot(1, 2, 2)\n",
    "xx, yy, x1, x2 = mesh2Dgrid((-2, -2), (2, 2), h)\n",
    "\n",
    "XXe = mapFeature(x_train.T[0], x_train.T[1], 2)  # Extend matrix for degree 2\n",
    "regression = LogisticRegression(max_iter=n, n_jobs=4, C=j, verbose=1, solver='lbfgs', random_state=0)\n",
    "regression.fit(XXe, y_train)\n",
    "\n",
    "XXe = mapFeature(x1, x2, 2)  # Extend matrix for degree 2\n",
    "\n",
    "p = regression.predict(XXe)\n",
    "clz_mesh = p.reshape(xx.shape)  # return to mesh format\n",
    "\n",
    "# Draw mesh and points.\n",
    "plti.pcolormesh(xx, yy, clz_mesh, cmap=cmap_light)\n",
    "for x_, y_ in zip(X.T, y):\n",
    "\tplti.scatter(x_[0], x_[1], color=getColor(y_), cmap=cmap_bold)\n",
    "\n",
    "plti.set_xlabel(\"Iterations\")\n",
    "plti.set_ylabel(\"Cost\")\n",
    "\n",
    "plt.suptitle(\"microchips.csv - Logistic Regression (Predefined)\")\n",
    "plt.show()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.T, y, test_size=0.25, random_state=0)\n",
    "\n",
    "for d in degress:\n",
    "\tpoly = PolynomialFeatures(d, include_bias=False)\n",
    "\tx1 = x_train.T[0]\n",
    "\tx2 = x_train.T[1]\n",
    "\tXXe = poly.fit_transform([[_x, _y] for _x, _y in zip(x1, x2)])\n",
    "\tx1 = x_test.T[0]\n",
    "\tx2 = x_test.T[1]\n",
    "\tXXeT = poly.fit_transform([[_x, _y] for _x, _y in zip(x1, x2)])\n",
    "\tregression = LogisticRegression()\n",
    "\n",
    "\tregression.fit(XXe, y_train)\n",
    "\tpredictions = regression.predict(XXeT)\n",
    "\tscore = regression.score(XXeT, y_test)\n",
    "\n",
    "\tprint(\"alpha {}, iterations: {} J(B) : {}\".format(alpha, n, round(combinationCost, 7)))\n",
    "\n",
    "\tplti = plt.subplot(3, 3, d)\n",
    "\tplti.set_title(\"Degree {} - score {}\".format(d, round(score, 4)))\n",
    "\n",
    "\t# Decision boundry\n",
    "\txx, yy, x1, x2 = mesh2Dgrid((-2, -2), (2, 2), h)\n",
    "\n",
    "\tpoly = PolynomialFeatures(d, include_bias=False)\n",
    "\tmap = poly.fit_transform([[_x, _y] for _x, _y in zip(x1, x2)])\n",
    "\n",
    "\tpredictedMap = regression.predict(map)\n",
    "\n",
    "\tclz_mesh = predictedMap.reshape(xx.shape)  # return to mesh format\n",
    "\tplti.pcolormesh(xx, yy, clz_mesh, cmap=cmap_light)\n",
    "\n",
    "\tfor _x, _y in zip(x_train, y_train):\n",
    "\t\tplti.scatter(_x[0], _x[1], color=getColor(_y), s=5)\n",
    "\tfor _x, _y in zip(x_test, y_test):\n",
    "\t\tplti.scatter(_x[0], _x[1], color=getColor(_y), s=10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
