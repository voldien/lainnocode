{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import struct\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from array import array as pyarray\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.ndimage import interpolation\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from KBisMean import bkmeans\n",
    "from exec2 import sammon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataSet0(size):\n",
    "    def moments(image):\n",
    "        \"\"\"\n",
    "        https://fsix.github.io/mnist/Deskewing.html\n",
    "        :param image:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        c0, c1 = np.mgrid[:image.shape[0], :image.shape[1]\n",
    "                          ]  # A trick in numPy to create a mesh grid\n",
    "        totalImage = np.sum(image)  # sum of pixels\n",
    "        m0 = np.sum(c0 * image) / totalImage  # mu_x\n",
    "        m1 = np.sum(c1 * image) / totalImage  # mu_y\n",
    "        m00 = np.sum((c0 - m0) ** 2 * image) / totalImage  # var(x)\n",
    "        m11 = np.sum((c1 - m1) ** 2 * image) / totalImage  # var(y)\n",
    "        m01 = np.sum((c0 - m0) * (c1 - m1) * image) / \\\n",
    "            totalImage  # covariance(x,y)\n",
    "        # Notice that these are \\mu_x, \\mu_y respectively\n",
    "        mu_vector = np.array([m0, m1])\n",
    "        # Do you see a similarity between the covariance matrix\n",
    "        covariance_matrix = np.array([[m00, m01], [m01, m11]])\n",
    "        return mu_vector, covariance_matrix\n",
    "\n",
    "    def deskew(image):\n",
    "        \"\"\"\n",
    "        https://fsix.github.io/mnist/Deskewing.html\n",
    "        :param image:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        c, v = moments(image)\n",
    "        alpha = v[0, 1] / v[0, 0]\n",
    "        affine = np.array([[1, 0], [alpha, 1]])\n",
    "        ocenter = np.array(image.shape) / 2.0\n",
    "        offset = c - np.dot(affine, ocenter)\n",
    "        return interpolation.affine_transform(image, affine, offset=offset)\n",
    "\n",
    "    def loadMNIST(imagePath, labelPath, size=1000, digits=np.arange(10)):\n",
    "        \"\"\"\n",
    "\n",
    "        :param imagePath:\n",
    "        :param labelPath:\n",
    "        :param size:\n",
    "        :param digits:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N = size\n",
    "\n",
    "        with gzip.open(labelPath, 'rb') as finf:\n",
    "            magic_nr, size = struct.unpack(\">II\", finf.read(8))\n",
    "            lbl = pyarray(\"b\", finf.read())\n",
    "\n",
    "            ind = [k for k in range(size) if lbl[k] in digits]\n",
    "            labels = np.zeros((N, 1), dtype=np.int8)\n",
    "            for i in range(N):\n",
    "                labels[i] = lbl[ind[i]]\n",
    "            finf.close()\n",
    "\n",
    "        with gzip.open(imagePath, 'rb') as fimg:\n",
    "            magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "            img = pyarray(\"B\", fimg.read())\n",
    "\n",
    "            ind = [k for k in range(size) if lbl[k] in digits]\n",
    "            images = np.zeros((N, rows * cols), dtype=np.float)\n",
    "\n",
    "            for i in range(N):  # int(len(ind) * size/100.)):\n",
    "                images[i] = np.array(img[ind[i] * rows * cols: (ind[i] + 1) * rows * cols]) \\\n",
    "                    .reshape((rows * cols)) / 255.0\n",
    "\n",
    "            fimg.close()\n",
    "\n",
    "        labels = [label[0] for label in labels]\n",
    "        return images, labels\n",
    "\n",
    "    MNISTImgTrain = \"./datasets/digits/train-images-idx3-ubyte.gz\"\n",
    "    MNISTLabelTrain = \"./datasets/digits/train-labels-idx1-ubyte.gz\"\n",
    "    MNISTImg = \"./datasets/digits/t10k-images-idx3-ubyte.gz\"\n",
    "    MNISTLabel = \"./datasets/digits/t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    X, y = loadMNIST(MNISTImgTrain, MNISTLabelTrain, size=size[0])\n",
    "    Xt, yt = loadMNIST(MNISTImg, MNISTLabel, size=size[1])\n",
    "\n",
    "    XSkewed = np.array(\n",
    "        [deskew(_x.reshape(28, 28)).reshape(X[0].shape) for _x in X])\n",
    "    tXSkewed = np.array(\n",
    "        [deskew(_x.reshape(28, 28)).reshape(X[0].shape) for _x in Xt])\n",
    "\n",
    "    return XSkewed, y, tXSkewed, yt, \"digits\"\n",
    "\n",
    "\n",
    "def dataSet1(size):\n",
    "    def loadMNIST(imagePath, labelPath, size=(1000, 100), digits=np.arange(10)):\n",
    "        \"\"\"\n",
    "\n",
    "        :param imagePath:\n",
    "        :param labelPath:\n",
    "        :param size:\n",
    "        :param digits:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N = size\n",
    "\n",
    "        with gzip.open(labelPath, 'rb') as finf:\n",
    "            magic_nr, size = struct.unpack(\">II\", finf.read(8))\n",
    "            lbl = pyarray(\"b\", finf.read())\n",
    "\n",
    "            ind = [k for k in range(size) if lbl[k] in digits]\n",
    "            labels = np.zeros((N, 1), dtype=np.int8)\n",
    "            for i in range(N):\n",
    "                labels[i] = lbl[ind[i]]\n",
    "            finf.close()\n",
    "\n",
    "        with gzip.open(imagePath, 'rb') as fimg:\n",
    "            magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "            img = pyarray(\"B\", fimg.read())\n",
    "\n",
    "            ind = [k for k in range(size) if lbl[k] in digits]\n",
    "            images = np.zeros((N, rows * cols), dtype=np.float)\n",
    "\n",
    "            for i in range(N):  # int(len(ind) * size/100.)):\n",
    "                images[i] = np.array(img[ind[i] * rows * cols: (ind[i] + 1) * rows * cols]) \\\n",
    "                    .reshape((rows * cols)) / 255.0\n",
    "\n",
    "            fimg.close()\n",
    "\n",
    "        labels = [label[0] for label in labels]\n",
    "        return images, labels\n",
    "\n",
    "    MNISTImgTrain = \"./datasets/fashion/train-images-idx3-ubyte.gz\"\n",
    "    MNISTLabelTrain = \"./datasets/fashion/train-labels-idx1-ubyte.gz\"\n",
    "    MNISTImg = \"./datasets/fashion/t10k-images-idx3-ubyte.gz\"\n",
    "    MNISTLabel = \"./datasets/fashion/t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    X, y = loadMNIST(MNISTImgTrain, MNISTLabelTrain, size=size[0])\n",
    "    Xt, yt = loadMNIST(MNISTImg, MNISTLabel, size=size[1])\n",
    "\n",
    "    # XSkewed = np.array([deskew(_x.reshape(28, 28)).reshape(X[0].shape) for _x in X])\n",
    "    # tXSkewed = np.array([deskew(_x.reshape(28, 28)).reshape(X[0].shape) for _x in Xt])\n",
    "\n",
    "    return X, y, Xt, yt, \"fashion\"\n",
    "\n",
    "\n",
    "def dataSet2(size):\n",
    "    print(\"Ignoring size request\")\n",
    "    df = pd.read_csv('datasets/leaf.csv', header=None).sample(frac=1)\n",
    "    #\n",
    "    X = np.array(df.drop(columns=[0, 1], axis=1), dtype=np.float)\n",
    "    y = np.array(df.loc[:, 0])\n",
    "\n",
    "    assert len(X) == len(y)\n",
    "\n",
    "    return X, y, None, None, 'Leaf'\n",
    "\n",
    "\n",
    "def loadDataSet(nth=0, training_size=1000, testing_size=100):\n",
    "    dat = [dataSet0, dataSet1, dataSet2]\n",
    "    return dat[nth](size=(training_size, testing_size))\n",
    "\n",
    "\n",
    "# Check all datasets\n",
    "def displaySampleData():\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(0, 3):\n",
    "        X, y, tx, ty, l = loadDataSet(i, training_size=10, testing_size=1)\n",
    "        for index, (image, label) in enumerate(zip(X[0:5], y[0:5])):\n",
    "            p_ = plt.subplot(3, 5, (i * 5) + (index + 1))\n",
    "            p_.imshow(np.reshape(image, (28, 28)), cmap=plt.cm.gray)\n",
    "            p_.set_title('Training: {}\\n'.format(label), fontsize=20)\n",
    "    plt.title(\"Example of data set elements\")\n",
    "    plt.suptitle(\"Examples\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126248/3820303732.py:62: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  images = np.zeros((N, rows * cols), dtype=np.float)\n",
      "/tmp/ipykernel_126248/3820303732.py:34: DeprecationWarning: Please use `affine_transform` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  return interpolation.affine_transform(image, affine, offset=offset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on dataset 'digits'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.decomposition.pca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb#ch0000002?line=18'>19</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb#ch0000002?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb#ch0000002?line=20'>21</a>\u001b[0m         pca \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb#ch0000002?line=21'>22</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/programming/projects/machineLearning/ML/legacy/assignment4/exec3.ipynb#ch0000002?line=22'>23</a>\u001b[0m     n_components \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.decomposition.pca'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAABjCAYAAACi5VNqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEz0lEQVR4nO3dT4gWdRzH8fcnzQIPCekhSjBJWjx00IfwFEEE6kEPddCLGcYiJZ2DDoGX8BRIkSwlZQeTPG1QRFDgSfNZKNOiWIPIEFwtvASW8O0wg23r7s7s15l9Zh8/L3jgmWf+fX/sh+eZeWaf7ygiMMu4Z9AF2NLl8Fiaw2NpDo+lOTyW5vBYWmV4JB2VdEXS+TnmS9JhSZOSzkna1HyZ1kV13nk+ALbOM38bsKF8jALv3nlZthRUhiciTgF/zLPITuBYFE4DqyQ91FSB1l1NHPM8DPw2bfpS+ZoNueWLuTNJoxQfbaxcuXLzyMjIYu7e5jAxMXE1ItYsdL0mwvM7sHba9CPla7eJiDFgDKDX60W/329g93anJP2aWa+Jj61xYE951rUFuB4RlxvYrnVc5TuPpOPA08BqSZeAN4B7ASLiCPAZsB2YBP4CXmyrWOuWyvBExO6K+QG80lhFtmT4G2ZLc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bG0WuGRtFXST2UPntdmmb9X0pSkb8vHS82Xal1T5xejy4B3gGcpOmCclTQeET/MWPRERBxooUbrqDrvPE8CkxHxS0T8DXxM0ZPH7nJ1wlO3/85zZVu5k5LWzjLfhkxTB8yfAusi4gngS+DD2RaSNCqpL6k/NTXV0K5tUOqEp7L/TkRci4gb5eR7wObZNhQRYxHRi4jemjUL7iVkHVMnPGeBDZIelbQC2EXRk+eWGT0IdwA/NleidVWdFis3JR0AvgCWAUcj4oKkg0A/IsaBVyXtAG5SNL/c22LN1hEa1C2T3FauOyRNRERvoev5G2ZLc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bG0pvrz3CfpRDn/jKR1jVdqnVMZnmn9ebYBG4HdkjbOWGwf8GdEPAa8BRxqulDrnqb68+zkv84YJ4FnJKm5Mq2LmurPc2uZiLgJXAcebKJA667KRgdNkjQKjJaTNySdX8z9t2A1cHXQRTTg8cxKdcJT2Z9n2jKXJC0HHgCuzdxQRIwBYwCS+pkf13fJMIwBinFk1mukP085/UL5/HngqxhU+w1bNE3153kf+EjSJEV/nl1tFm3dMLD+PJJGy4+xJWsYxgD5cQwsPLb0+fKEpbUenmG4tDEMt0+QdFTSlbm+HlHhcDnGc5I2VW40Ilp7UBxgXwTWAyuA74CNM5Z5GThSPt9FcRuCVutqYQx7gbcHXWvFOJ4CNgHn55i/HfgcELAFOFO1zbbfeYbh0sZQ3D4hIk5RnAnPZSdwLAqngVUzWiTfpu3wDMOljbvl9gl1x3mLD5ibUev2CcOm7fAs5NIG813aGKDGbp/QcXX+Vv/TdniG4dLG3XL7hHFgT3nWtQW4HhGX511jEY7ytwM/U5yxvF6+dhDYUT6/H/gEmAS+AdYP+swkMYY3gQsUZ2JfAyODrnmWMRwHLgP/UBzP7AP2A/vL+aL4p7+LwPdAr2qb/obZ0nzAbGkOj6U5PJbm8Fiaw2NpDo+lOTyW5vBY2r9qhjFg7o9qAgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# displaySampleData()\n",
    "\n",
    "drPickles = ['exec3_{}_DR_model.sav'.format(i) for i in range(0, 10)]\n",
    "ClusterPickles = ['exec3_{}_CL_model.sav'.format(i) for i in range(0, 10)]\n",
    "\n",
    "# DR techniques\n",
    "for i in range(0, 3):  # Iterate data sets.\n",
    "    X, y, tx, ty, name = loadDataSet(i, 500, 50)\n",
    "    n_samples = len(X)\n",
    "    n_features = len(X[0])\n",
    "\n",
    "    print(\"Starting on dataset '{}'\".format(name))\n",
    "    for j in range(0, 3):  # Iterate DR techniques.\n",
    "        filename = drPickles[i * 3 + j]\n",
    "        plti = plt.subplot(3, 3, (i * 3) + j + 1)\n",
    "        if j == 0:  # PCA\n",
    "\n",
    "            # Generate new PCA or load.\n",
    "            try:\n",
    "                with open(filename, 'rb') as f:\n",
    "                    pca = pickle.load(f)\n",
    "            except FileNotFoundError as f:\n",
    "                n_components = min(n_samples, n_features)\n",
    "                pca = PCA(n_components=3)\n",
    "                pca.fit(X, y)\n",
    "                pickle.dump(pca, open(filename, 'wb'))\n",
    "\n",
    "            pca_result = pca.transform(X)\n",
    "            ax = plti\n",
    "\n",
    "            pca0 = pca_result[:, 0]\n",
    "            pca1 = pca_result[:, 1]\n",
    "            pca2 = pca_result[:, 2]\n",
    "\n",
    "            ax.scatter(\n",
    "                x=pca0, y=pca1,\n",
    "                c=y,\n",
    "                alpha=0.3)\n",
    "            ax.set_xlabel('PCA-one')\n",
    "            ax.set_ylabel('PCA-two')\n",
    "            plti.set_title(\"PCA - {}\".format(name))\n",
    "\n",
    "        elif j == 1:  # t-SNE\n",
    "            colors = ['r', 'g', 'b', 'c', 'm',\n",
    "                      'y', 'k', 'w', 'orange', 'purple']\n",
    "\n",
    "            # Generate new t-SNE or load.\n",
    "            try:\n",
    "                with open(filename, 'rb') as f:\n",
    "                    tsne = pickle.load(f)\n",
    "            except FileNotFoundError as f:\n",
    "                tsne = TSNE(n_components=2, random_state=0,\n",
    "                            verbose=1, perplexity=40, n_iter=300)\n",
    "                tsne.fit(X)\n",
    "                pickle.dump(tsne, open(filename, 'wb'))\n",
    "\n",
    "            tsne_results = tsne.fit_transform(X)\n",
    "            tsne0 = tsne_results[:, 0]\n",
    "            tsne1 = tsne_results[:, 1]\n",
    "\n",
    "            target_ids = range(len(y))\n",
    "            for _i, label in zip(target_ids, y):\n",
    "                plti.scatter(x=tsne0, y=tsne1,\n",
    "                             c=y,\n",
    "                             alpha=0.3)\n",
    "            plti.set_title(\"t-SNE - {}\".format(name))\n",
    "\n",
    "        elif j == 2:  # Sammon mapping.\n",
    "            result = sammon(X, 250, 0.035, 0.1)\n",
    "            sammon0 = result[:, 0]\n",
    "            sammon1 = result[:, 1]\n",
    "            plti.scatter(x=sammon0, y=sammon1,\n",
    "                         c=y,\n",
    "                         alpha=0.3)\n",
    "            plti.set_title(\"Sammon mapping - {}\".format(name))\n",
    "\n",
    "plt.title(\"Dimension Reduction\")\n",
    "plt.suptitle(\"Dimension Reduction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cluster techniques\n",
    "# Bisecting k-Means with classic k-Means and hierarchical clustering for each data set\n",
    "for i in range(0, 3):\n",
    "    X, y, tx, ty, name = loadDataSet(i)\n",
    "    pcaModelFilePath = ClusterPickles[i]\n",
    "    k = 5\n",
    "    try:\n",
    "        with open(pcaModelFilePath, 'rb') as f:\n",
    "            pca = pickle.load(f)\n",
    "    except Exception as err:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X)\n",
    "        pickle.dump(pca, open(pcaModelFilePath, 'wb'))\n",
    "\n",
    "    pca_result = pca.fit_transform(X)\n",
    "    #pca_test = pca.transform(tx)\n",
    "\n",
    "    pca0 = pca_result[:, 0]\n",
    "    pca1 = pca_result[:, 1]\n",
    "\n",
    "    for j in range(0, 3):\n",
    "        plti = plt.subplot(3, 3, i * 3 + j + 1)\n",
    "        if j == 0:\n",
    "            plti.set_title(\"Hierarchical Clustering - {}\".format(name))\n",
    "            clus = AgglomerativeClustering(n_clusters=k)\n",
    "            clus.fit(pca_result, y)\n",
    "            plti.scatter(x=pca0, y=pca1, c=clus.labels_, alpha=0.3)\n",
    "            #l = clus.fit_predict(pca_test)\n",
    "        elif j == 1:\n",
    "            plti.set_title(\"K Mean - {}\".format(name))\n",
    "            kmean = KMeans(n_jobs=-1, n_clusters=k, random_state=True)\n",
    "            kmean.fit(pca_result, y)\n",
    "            plti.scatter(x=pca0, y=pca1, c=kmean.labels_, alpha=0.3)\n",
    "        elif j == 2:\n",
    "            plti.set_title(\"Bisecting K Mean - {}\".format(name))\n",
    "            labels = bkmeans(pca_result, k, 30)\n",
    "            plti.scatter(x=pca0, y=pca1, c=labels, alpha=0.3)\n",
    "\n",
    "plt.title(\"Cluster Reduction\")\n",
    "plt.suptitle(\"Cluster Reduction\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9705cd9bb1cd3f75cacc64c830c816f4d5b8b46bb8973c8b095f871cd13babda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
