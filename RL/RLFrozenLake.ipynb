{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforment Learning Snake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tensorflow-gpu gym keras keras-rl2 pyvirtualdisplay pyglet pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from envs import SnakeWorldEnv\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym import core, spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvirtualdisplay\n",
    "import PIL.Image\n",
    "from gym.wrappers import RecordVideo\n",
    "import tensorflow as tf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the graphic window to exist without having to rendered to the display. Aka add support for headless graphic, when using a OpenGL rendering library.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1280, 720)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeWorldEnv(Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 15}\n",
    "    metadata = {\"render_modes\": [\"human\", \"ansi\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    SCREEN_SCALE = 30.0\n",
    "    SCREEN_DIM = 28 * SCREEN_SCALE\n",
    "\n",
    "    def __init__(self, nrObsticles=4, world_size=(28, 28)):\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.world_size = (28, 28)\n",
    "\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(2, 1), dtype=np.float32)\n",
    "\n",
    "        # World Space\n",
    "        self.state = np.zeros(shape=world_size).astype(dtype='float32')\n",
    "\n",
    "        self.snake_length = 3\n",
    "        self.snake_head_position = (0, 0)\n",
    "        self.apple_position = (0, 0)\n",
    "        self.last_time_grown = 0\n",
    "        self.snake_body = []\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        apple_consumed = False\n",
    "\n",
    "        current_distance_from_apple = np.linalg.norm(self.apple_position - self.snake_head_position)\n",
    "        # Update params.\n",
    "        # Apply action\n",
    "        # Remove head from previous.\n",
    "        self.state[int(self.snake_head_position[0]), int(self.snake_head_position[1])] = 0\n",
    "        # Move the last sequence to the next position.\n",
    "        if action == 0:\n",
    "            self.snake_head_position += np.array([1, 0])\n",
    "        if action == 1:\n",
    "            self.snake_head_position += np.array([-1, 0])\n",
    "        if action == 2:\n",
    "            self.snake_head_position += np.array([0, 1])\n",
    "        if action == 3:\n",
    "            self.snake_head_position += np.array([0, -1])\n",
    "\n",
    "        if self.snake_head_position[0] >= self.state.shape[0] or self.snake_head_position[0] < 0:\n",
    "            done = True\n",
    "            reward = 0\n",
    "        elif self.snake_head_position[1] >= self.state.shape[1] or self.snake_head_position[1] < 0:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if not done:\n",
    "            # Update new position\n",
    "            self.state[int(self.snake_head_position[0]), int(self.snake_head_position[1])] = 255\n",
    "\n",
    "            #\n",
    "            self.last_time_grown += 1\n",
    "\n",
    "            # If snake overlap.\n",
    "            next_distance_from_apple = np.linalg.norm(self.apple_position - self.snake_head_position)\n",
    "\n",
    "            reward = (1.0 / max(1.0, np.linalg.norm(self.apple_position - self.snake_head_position))) * 0.25\n",
    "            # Reward negative going away.\n",
    "            if next_distance_from_apple > current_distance_from_apple:\n",
    "                reward = -reward\n",
    "\n",
    "            if self.state[self.apple_position[0], self.apple_position[1]] == 255:\n",
    "                apple_consumed = True\n",
    "            # Calculate reward\n",
    "            # If closer. small reward, get apple. big reward.\n",
    "            if apple_consumed:\n",
    "                reward += 20\n",
    "                self.snake_length += 1\n",
    "                self.last_time_grown = 0\n",
    "\n",
    "                self._reset_level()\n",
    "\n",
    "                # Update apple position\n",
    "                self.state[self.apple_position[0], self.apple_position[1]] = 0\n",
    "                self.apple_position = np.random.randint(size=(2,), low=0, high=28)\n",
    "                self.state[self.apple_position[0], self.apple_position[1]] = -255\n",
    "            else:\n",
    "                # Check if timeout.\n",
    "                if self.last_time_grown >= 500 - self.snake_length:\n",
    "                    done = True\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    done = False\n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        Appledir = (self.apple_position - self.snake_head_position)\n",
    "        distance = np.linalg.norm(Appledir)\n",
    "        if distance > 0:\n",
    "            Appledir = Appledir / np.linalg.norm(Appledir)\n",
    "        else:\n",
    "            Appledir = np.array([0, 0])\n",
    "\n",
    "        # Return step information\n",
    "        return Appledir, reward, done, info\n",
    "\n",
    "    def _reset_level(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake_length = 3\n",
    "        self.snake_head_position = np.asarray(self.state.shape, dtype=int) / 2\n",
    "        self.snake_last_position = np.asarray(self.snake_head_position - (0, 3))\n",
    "\n",
    "        self.state = np.zeros(shape=self.state.shape).astype(dtype='float32')\n",
    "\n",
    "#        for x in range(0, self.snake_length):\n",
    "        self.state[int(self.snake_head_position[0]), int(self.snake_head_position[1])] = 255\n",
    "\n",
    "        self.apple_position = np.random.randint(size=(2,), low=0, high=28)\n",
    "        self.state[self.apple_position[0], self.apple_position[1]] = -255\n",
    "        self.last_time_grown = 0\n",
    "\n",
    "        Appledir = (self.apple_position - self.snake_head_position)\n",
    "        distance = np.linalg.norm(Appledir)\n",
    "        if distance > 0:\n",
    "            Appledir = Appledir / np.linalg.norm(Appledir)\n",
    "        else:\n",
    "            Appledir = np.array([0, 0])\n",
    "\n",
    "        #result = np.squeeze(np.array(self.state.flatten(), dtype=np.float32))\n",
    "\n",
    "        self._reset_level()\n",
    "\n",
    "        return np.array([0, 0])\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            pygame.display.set_caption(\"Frozen Lake\")\n",
    "            if mode == \"human\":\n",
    "                self.window_surface = pygame.display.set_mode((self.SCREEN_DIM, self.SCREEN_DIM))\n",
    "            else:  # rgb_array\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        # Load asset if not loaded.\n",
    "\n",
    "        # Draw enviroment.\n",
    "\n",
    "        self.surf = pygame.Surface((self.SCREEN_DIM, self.SCREEN_DIM))\n",
    "        self.surf.fill((128, 128, 128))\n",
    "        s = self.state\n",
    "\n",
    "        # pygame.draw.\n",
    "        pygame.draw.circle(self.surf, (0, 255, 0), np.asarray(self.apple_position) * self.SCREEN_SCALE, self.SCREEN_SCALE * 0.5)\n",
    "        pygame.draw.circle(self.surf, (255, 0, 0), np.asarray(self.snake_head_position) * self.SCREEN_SCALE, self.SCREEN_SCALE * 0.5)\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        if mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "        else:\n",
    "            return self.isopen\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeWorldEnv(world_size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Of Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=840x840>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render(mode=\"rgb_array\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) int64\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation.shape, observation.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Tabular\n",
    "The simple as well the naive approuch for deterministic spaces. \n",
    "\n",
    "$$Q(S,A_t) = Q(S,A_t) + \\alpha (r + \\gamma \\max_a Q(S',a) ) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Recorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"This wrapper converts a Box observation into a single integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins=10, low=None, high=None):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, Box)\n",
    "\n",
    "        low = self.observation_space.low if low is None else low\n",
    "        high = self.observation_space.high if high is None else high\n",
    "\n",
    "        self.n_bins = n_bins\n",
    "        self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in\n",
    "                         zip(low.flatten(), high.flatten())]\n",
    "        self.observation_space = Discrete(n_bins ** low.flatten().shape[0])\n",
    "\n",
    "    def _convert_to_one_number(self, digits):\n",
    "        return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        digits = [np.digitize([x], bins)[0]\n",
    "                  for x, bins in zip(observation.flatten(), self.val_bins)]\n",
    "        return self._convert_to_one_number(digits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, learning_rate, n_steps,epsilon=0.1, gamma=0.99):\n",
    "\n",
    "    Q = defaultdict(float)\n",
    "    gamma = 0.99  # Discounting factor\n",
    "    alpha = 0.5  # soft update param\n",
    "\n",
    "    def update_Q(s, r, a, s_next, done):\n",
    "        max_q_next = max([Q[s_next, a] for a in actions]) \n",
    "        # Do not include the next state's value if currently at the terminal state.\n",
    "        Q[s, a] += alpha * (r + gamma * max_q_next * (1.0 - done) - Q[s, a])\n",
    "\n",
    "    n_steps = 100000\n",
    "\n",
    "    def act(ob):\n",
    "        if np.random.random() < epsilon:\n",
    "            # action_space.sample() is a convenient function to get a random action\n",
    "            # that is compatible with this given action space.\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # Pick the action with highest q value.\n",
    "        qvals = {a: Q[ob, a] for a in actions}\n",
    "        max_q = max(qvals.values())\n",
    "        # In case multiple actions have the same maximum q value.\n",
    "        actions_with_max_q = [a for a, q in qvals.items() if q == max_q]\n",
    "        return np.random.choice(actions_with_max_q)\n",
    "    \n",
    "    ob = env.reset()\n",
    "    rewards = []\n",
    "    reward = 0.0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        a = act(ob)\n",
    "        ob_next, r, done, _ = env.step(a)\n",
    "        update_Q(ob, r, a, ob_next, done)\n",
    "        reward += r\n",
    "\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            reward = 0.0\n",
    "            ob = env.reset()\n",
    "        else:\n",
    "            ob = ob_next\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiscreteEnv = DiscretizedObservationWrapper(\n",
    "    env,\n",
    "    n_bins=8,\n",
    "    low=[-2.4, -2.0, -0.42, -3.5],\n",
    "    high=[2.4, 2.0, 0.42, 3.5]\n",
    ")\n",
    "hyperparamers = {\"steps\": [10000,50000,100000], \"learning_rate\": [0.001,0.005], \"epsilon\" : [0.15,0.1,0.5, 0.1 0.05], \"gamma\": [0.9, 0.95, 0.99, 1.0] }\n",
    "\n",
    "DiscreteEnv = RecordVideo(DiscreteEnv, video_folder='FrozenLake/QLearning', step_trigger=lambda e: True)\n",
    "DiscreteEnv.start_video_recorder()\n",
    "\n",
    "for steps in hyperparamers.steps:\n",
    "    for learning_rate in hyperparamers.learning_rate:\n",
    "        for epsilon in hyperparamers.epsilon:\n",
    "            for gamma in hyperparamers.gamma:\n",
    "                rewards = QLearning(DiscreteEnv,n_steps=steps,learning_rate=learning_rate,epsilon=epsilon,gamma=gamma)\n",
    "\n",
    "            # Generate result data\n",
    "\n",
    "\n",
    "DiscreteEnv.close_video_recorder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNQ Learning - Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 6\n",
    "def build_model(states, actions):\n",
    "    model = Sequential(name=\"\")\n",
    "    # Sum all dims to the final\n",
    "    model.add(keras.layers.Input((WINDOW_LENGTH, np.prod(states))))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization(dtype='float32'))\n",
    "\n",
    "    model.add(Dense(units=16, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization(dtype='float32'))\n",
    "    model.add(Dense(units=actions, activation='linear'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 12)                0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                416       \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,204\n",
      "Trainable params: 1,108\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "del model \n",
    "model = build_model(env.observation_space.shape, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent, DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, EpisodeParameterMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "\n",
    "    GAMMA = 0.98           # discount factor\n",
    "    BUFFER_SIZE = 100000   # replay buffer size\n",
    "    BATCH_SIZE = 64        # Update batch size\n",
    "    LR = 0.0001            # learning rate\n",
    "    TAU = 1e-3             # for soft update of target parameters\n",
    "    UPDATE_EVERY = 1       # how often to update the network\n",
    "    UPDATE_TARGET = 10000  # After which thershold replay to be started\n",
    "    EPS_START = 0.99       # starting value of epsilon\n",
    "    EPS_END = 0.01         # Ending value of epsilon\n",
    "    EPS_DECAY = 100         # Rate by which epsilon to be decayed\n",
    "\n",
    "    policy = BoltzmannQPolicy()#(clip=(0., 1.))\n",
    "\n",
    "    memory = SequentialMemory(limit=50000, window_length=WINDOW_LENGTH)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, enable_double_dqn=False, enable_dueling_network=False,\n",
    "                   nb_actions=actions, nb_steps_warmup=1000, target_model_update=1e-2, gamma=GAMMA)\n",
    "    return dqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    dqn = build_agent(model, actions)\n",
    "    dqn.compile(Adam(learning_rate=8e-3), metrics=['mae'])\n",
    "\n",
    "    dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "\n",
    "env.close_video_recorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional DNQ Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9705cd9bb1cd3f75cacc64c830c816f4d5b8b46bb8973c8b095f871cd13babda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
